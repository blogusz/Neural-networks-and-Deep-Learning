### Lab1 - Basic matrix operations in NumPy. Visualization of neural network weights.

1. Implementation of the sigmoid activation function.
2. Implementation of feed-forward operation in a single-layer neural network.
3. Implementation of the tiles function for visualizing weights in a neural network layer.

### Lab2 - Data visualization and classification with scikit-learn and matplotlib.

### Lab3 - Contrastive Divergence Algorithm.

1. Implementation of the reconstruction error in Restricted Boltzmann Machines.
2. Implementation of the CD-k algorithm.

### Lab4 - Persistent Contrastive Divergence Algorithm.

1. Implementation of the PCD algorithm.

### Lab5 - Momentum Method. Deep Belief Networks.

1. Implementation of the momentum method in CD-k.
2. Implementation of training and sampling in Deep Belief Networks.

### Lab6 - Backpropagation Algorithm.

1. Implementation of the backpropagation algorithm.

### Lab7 - Regularization and pre-training in deep networks.

1. Implementation of L1 and L2 costs in RBM.
2. Pre-training of deep MLP.

### Lab8 - ReLU activation function. Regularization with weight vector norm.

1. Implementation of a limit on the weight vector norm.
2. Deep neural networks with ReLU activation function.

### Lab9 - Dropout.

1. Implementation of Dropout regularization.

### Lab10 - Autoassociative networks.

1. RBM with Gaussian units.
2. Implementation of an autoassociative network with a linear encoding layer.

### Lab11 - Nesterov method and data visualization with autoassociative networks.

1. Implementation of the Nesterov method in an autoassociative network.
2. Visualization of the MNIST dataset using an autoassociative network.

### Lab12 - Convolutional Neural Networks.

1. Implementation of a convolutional neural network.

### Lab13 - Negative Sampling Algorithm.

1. Implementation of the Negative Sampling algorithm.
